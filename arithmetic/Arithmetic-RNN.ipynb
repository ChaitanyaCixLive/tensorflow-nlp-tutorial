{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x10bed4d90>> ignored\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "# tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7d453ddc238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_arithmetic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nlocascio/MIT/MEng/tensorflow-nlp-tutorial/arithmetic/utils.pyc\u001b[0m in \u001b[0;36mload_arithmetic_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurrent_char_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequations_of_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_char_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtoken_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_char_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "X, y, index_to_word, equations, answers, max_length = utils.load_arithmetic_data()\n",
    "X_train, y_train, X_test, y_test = utils.split_data(X, y)\n",
    "vocab_size = len(index_to_word)\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "s_i = 50\n",
    "print(\"Equation:\", equations[s_i])\n",
    "print(\"Answer:\", answers[s_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_placeholder = tf.placeholder(tf.float32, shape=(None, max_length, vocab_size), name='data_placeholder')\n",
    "labels_placeholder = tf.placeholder(tf.float32, shape=(None, max_length, vocab_size), name='labels_placeholder')\n",
    "keep_prob_placeholder = tf.placeholder(tf.float32, name='keep_prob_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n",
    "                scope=None):\n",
    "  \"\"\"RNN decoder for the sequence-to-sequence model.\n",
    "  Args:\n",
    "    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
    "    cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "    loop_function: If not None, this function will be applied to the i-th output\n",
    "      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "      except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "      but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "      Signature -- loop_function(prev, i) = next\n",
    "        * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "        * i is an integer, the step number (when advanced control is needed),\n",
    "        * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"rnn_decoder\".\n",
    "  Returns:\n",
    "    A tuple of the form (outputs, state), where:\n",
    "      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "        shape [batch_size x output_size] containing generated outputs.\n",
    "      state: The state of each cell at the final time-step.\n",
    "        It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n",
    "         states can be the same. They are different for LSTM cells though.)\n",
    "  \"\"\"\n",
    "  with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    prev = None\n",
    "    for i, inp in enumerate(decoder_inputs):\n",
    "      if loop_function is not None and prev is not None:\n",
    "        with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "          inp = loop_function(prev, i)\n",
    "      if i > 0:\n",
    "        variable_scope.get_variable_scope().reuse_variables()\n",
    "      output, state = cell(inp, state)\n",
    "      outputs.append(output)\n",
    "      if loop_function is not None:\n",
    "        prev = output\n",
    "  return outputs, state\n",
    "\n",
    "def linear(input_, output_size, layer_scope, stddev=0.02, bias_start=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(layer_scope, reuse=True):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "            initializer=tf.constant_initializer(bias_start))\n",
    "        return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "name_scope() got an unexpected keyword argument 'reuse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1da026e51a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder_layers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Create LSTM Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rnn_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: name_scope() got an unexpected keyword argument 'reuse'"
     ]
    }
   ],
   "source": [
    "# Define Computation Graph\n",
    "n_rnn_layers = 3\n",
    "n_fc_layers = 2\n",
    "n_rnn_nodes = 256\n",
    "n_fc_nodes = 128\n",
    "batch_size = 64\n",
    "\n",
    "with tf.name_scope(\"encoder_layers\") as scope:\n",
    "    # Create LSTM Cell\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_rnn_nodes, state_is_tuple=False)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell, output_keep_prob=keep_prob_placeholder)\n",
    "    stacked_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * n_rnn_layers, state_is_tuple=False)\n",
    "    output, encoding = tf.nn.dynamic_rnn(stacked_cells, data_placeholder, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"decoder_layers\") as scope:\n",
    "    # Connect RNN Embedding output to decoder\n",
    "    initial_state = tf.zeros([batch_size, n_rnn_nodes], tf.float32)\n",
    "    encoder_cell = tf.nn.rnn_cell.LSTMCell(n_rnn_nodes, state_is_tuple=False)\n",
    "    encoder_list = [encoder for i in range(max_length)]\n",
    "    outputs, state = tf.nn.seq2seq.rnn_decoder(encoder, initial_state, encoder_cell)\n",
    "    \n",
    "    fc0_output = [tf.nn.relu(linear(data_placeholder, n_hidden_units_h0, 'hidden0')) for output in outputs]\n",
    "    output_shape = [-1, self.n_features, self.sequence_length, 2]\n",
    "\n",
    "    predictions = tf.reshape(fc2_output, output_shape)\n",
    "    fc_final = linear(fc_prev, n_classes, 'fc{}'.format(n_fc_layers-1))\n",
    "\n",
    "logits = tf.nn.softmax(fc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Loss Function + Optimizer\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder))\n",
    "\n",
    "weights = [tf.constant(1.0, shape=[2]) for i in range(3)]\n",
    "loss = sequence_loss(logits, label_placeholder, weights)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0002).minimize(loss)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "prediction_is_correct = tf.equal(\n",
    "    tf.argmax(logits, 1), tf.argmax(labels_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train loss at step 0 : 1.09861\n",
      "Minibatch train accuracy: 0.469%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.468%\n",
      "Minibatch train loss at step 2 : 1.0986\n",
      "Minibatch train accuracy: 0.438%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.431%\n",
      "Minibatch train loss at step 4 : 1.0986\n",
      "Minibatch train accuracy: 0.469%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.483%\n",
      "Minibatch train loss at step 6 : 1.0986\n",
      "Minibatch train accuracy: 0.469%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.565%\n",
      "Minibatch train loss at step 8 : 1.0986\n",
      "Minibatch train accuracy: 0.406%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.567%\n",
      "Minibatch train loss at step 10 : 1.09859\n",
      "Minibatch train accuracy: 0.500%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.557%\n",
      "Minibatch train loss at step 12 : 1.0986\n",
      "Minibatch train accuracy: 0.438%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.542%\n",
      "Minibatch train loss at step 14 : 1.09862\n",
      "Minibatch train accuracy: 0.188%\n",
      "Test loss: 1.099\n",
      "Test accuracy: 0.506%\n",
      "Minibatch train loss at step"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "\n",
    "num_steps = 1000\n",
    "batch_size = 32\n",
    "keep_prob_rate = 0.75\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "for step in xrange(num_steps):\n",
    "    offset = (step * batch_size) % (X_train.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = X_train[offset:(offset + batch_size), :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    # We built our networking using placeholders. It's like we've made reservations for a party of 6.\n",
    "    # So use feed_dict to fill what we reserved. And we can't show up with 9 people. \n",
    "\n",
    "    feed_dict_train = {data_placeholder: batch_data, labels_placeholder : batch_labels, keep_prob_placeholder: keep_prob_rate}\n",
    "    # Run the optimizer, get the loss, get the predictions.\n",
    "    # We can run multiple things at once and get their outputs\n",
    "    _, loss_value_train, predictions_value_train, accuracy_value_train = session.run(\n",
    "      [optimizer, loss, prediction, accuracy], feed_dict=feed_dict_train)\n",
    "    if (step % 2 == 0):\n",
    "        print \"Minibatch train loss at step\", step, \":\", loss_value_train\n",
    "        print \"Minibatch train accuracy: %.3f%%\" % accuracy_value_train\n",
    "        feed_dict_test = {data_placeholder: X_test, labels_placeholder: y_test, keep_prob_placeholder: 1.0}\n",
    "        loss_value_test, predictions_value_test, accuracy_value_test = session.run(\n",
    "            [loss, prediction, accuracy], feed_dict=feed_dict_test)\n",
    "        print \"Test loss: %.3f\" % loss_value_test\n",
    "        print \"Test accuracy: %.3f%%\" % accuracy_value_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
